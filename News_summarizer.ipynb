{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nithin2211/News-Summariser/blob/main/News_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install exact package versions and spacy model\n",
        "!pip install -q -U pip setuptools==68.0.0 wheel\n",
        "!pip install -q transformers sentence-transformers torch spacy sumy rouge-score scikit-learn requests matplotlib wordcloud\n",
        "!pip install -q git+https://github.com/csurfer/rake-nltk.git\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9Xfq2eGAP-o",
        "outputId": "e1ec3e0a-c187-4a78-d45c-d233cc6ca25f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/804.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/804.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for breadability (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rake-nltk (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "from rake_nltk import Rake\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# NLTK setup\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Added this line\n",
        "\n",
        "# SpaCy setup\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Set NewsAPI environment key\n",
        "NEWSAPI_KEY = input(\"Enter your NewsAPI key: \").strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COyeS-frAdDh",
        "outputId": "bd6ac2da-b32a-4d5f-e9a6-d19955abba36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORIES = ['technology', 'sports', 'business', 'health', 'entertainment']\n",
        "ARTICLES_PER_CAT = 30\n",
        "DATA_CACHE = 'newsapi_cache.json'\n",
        "\n",
        "def fetch_news(category, api_key, page_size=30, retries=3):\n",
        "    url = f'https://newsapi.org/v2/top-headlines?category={category}&pageSize={page_size}&apiKey={api_key}&language=en'\n",
        "    for attempt in range(retries):\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()['articles']\n",
        "        time.sleep(2 + attempt)\n",
        "    return []\n",
        "\n",
        "# Load cached data if exists & valid\n",
        "if os.path.exists(DATA_CACHE):\n",
        "    with open(DATA_CACHE, 'r') as f:\n",
        "        cached = json.load(f)\n",
        "    if all(cat in cached for cat in CATEGORIES):\n",
        "        news_data = cached\n",
        "    else:\n",
        "        news_data = {}\n",
        "else:\n",
        "    news_data = {}\n",
        "\n",
        "# Fetch missing categories with retry\n",
        "for cat in CATEGORIES:\n",
        "    if cat not in news_data or len(news_data[cat]) < ARTICLES_PER_CAT:\n",
        "        fetched = fetch_news(cat, NEWSAPI_KEY, ARTICLES_PER_CAT)\n",
        "        news_data[cat] = fetched\n",
        "\n",
        "# Save to cache\n",
        "with open(DATA_CACHE, 'w') as f:\n",
        "    json.dump(news_data, f)\n",
        "print(\"Fetched articles per category:\", {cat: len(news_data[cat]) for cat in CATEGORIES})\n"
      ],
      "metadata": {
        "id": "EHs7Avi2AsTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase, remove non-alpha characters\n",
        "    text = ''.join([ch.lower() if ch.isalpha() or ch.isspace() else ' ' for ch in str(text)])\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    stemmed = [stemmer.stem(t) for t in tokens]\n",
        "    lemmatized = [lemmatizer.lemmatize(t) for t in stemmed]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "# Preprocess articles\n",
        "for cat in CATEGORIES:\n",
        "    for article in news_data[cat]:\n",
        "        # Ensure all parts are strings before concatenation\n",
        "        title = article.get('title') or ''\n",
        "        description = article.get('description') or ''\n",
        "        content = article.get('content') or ''\n",
        "        article['cleaned_text'] = preprocess_text(f\"{title}. {description}. {content}\")"
      ],
      "metadata": {
        "id": "l0UMG8jyAzP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Extractive Summarization: TextRank via sumy\n",
        "def textrank_summary(text, num_sentences=3):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = TextRankSummarizer()\n",
        "    summary = summarizer(parser.document, num_sentences)\n",
        "    return ' '.join(str(sentence) for sentence in summary)\n",
        "\n",
        "# Abstractive Summarization: BART\n",
        "bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "def bart_summary(text, min_length=40, max_length=130):\n",
        "    try:\n",
        "        summary = bart_summarizer(text, min_length=min_length, max_length=max_length)[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        summary = \"\"\n",
        "    return summary\n",
        "\n",
        "# Hybrid: TextRank then BART\n",
        "def hybrid_summary(text):\n",
        "    intermediate = textrank_summary(text, num_sentences=5)\n",
        "    return bart_summary(intermediate)\n",
        "\n",
        "# Apply summarization for first article per category\n",
        "hybrid_summaries = {}\n",
        "for cat in CATEGORIES:\n",
        "    article = news_data[cat][0]\n",
        "    text = article['cleaned_text']\n",
        "    extractive = textrank_summary(text, num_sentences=3)\n",
        "    abstractive = bart_summary(text, min_length=40, max_length=130)\n",
        "    hybrid = hybrid_summary(text)\n",
        "    article['summaries'] = {'extractive': extractive, 'abstractive': abstractive, 'hybrid': hybrid}\n",
        "    hybrid_summaries[cat] = hybrid"
      ],
      "metadata": {
        "id": "8dku8hh1BZ07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAKE keyword extraction\n",
        "def extract_rake_keywords(text, top_n=10):\n",
        "    rake = Rake()\n",
        "    rake.extract_keywords_from_text(text)\n",
        "    return rake.get_ranked_phrases()[:top_n]\n",
        "\n",
        "# TF-IDF keyword extraction over all articles\n",
        "all_texts = [art['cleaned_text'] for cat in CATEGORIES for art in news_data[cat]]\n",
        "vectorizer = TfidfVectorizer(max_features=50, stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "def extract_tfidf_keywords(text, top_n=10):\n",
        "    response = vectorizer.transform([text])\n",
        "    sorted_indices = np.argsort(response.toarray()[0])[::-1][:top_n]\n",
        "    return [feature_names[i] for i in sorted_indices]\n",
        "\n",
        "# Store top keywords for articles\n",
        "for cat in CATEGORIES:\n",
        "    for article in news_data[cat]:\n",
        "        text = article['cleaned_text']\n",
        "        article['keywords'] = {\n",
        "            'rake': extract_rake_keywords(text, 10),\n",
        "            'tfidf': extract_tfidf_keywords(text, 10)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "xQEWb7SnCjml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ner(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "# Example NER for first article each category\n",
        "ner_samples = {}\n",
        "for cat in CATEGORIES:\n",
        "    article = news_data[cat][0]\n",
        "    ner_entities = get_ner(article['cleaned_text'])\n",
        "    article['ner'] = ner_entities\n",
        "    ner_samples[cat] = ner_entities\n"
      ],
      "metadata": {
        "id": "PjGB3-kZCpug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare labeled dataset from NewsAPI (based on categories)\n",
        "texts = []\n",
        "labels = []\n",
        "for cat in CATEGORIES:\n",
        "    for article in news_data[cat]:\n",
        "        txt = article['cleaned_text']\n",
        "        if txt:\n",
        "            texts.append(txt)\n",
        "            labels.append(cat)\n",
        "\n",
        "# TF-IDF features\n",
        "tfidf = TfidfVectorizer(max_features=200, stop_words='english')\n",
        "X = tfidf.fit_transform(texts)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split for baseline\n",
        "idx = np.arange(len(y))\n",
        "np.random.seed(42)\n",
        "train_idx = idx[:int(0.7*len(y))]\n",
        "test_idx = idx[int(0.7*len(y)):]\n",
        "\n",
        "clf = LogisticRegression(max_iter=300)\n",
        "clf.fit(X[train_idx], y[train_idx])\n",
        "\n",
        "y_pred = clf.predict(X[test_idx])\n",
        "\n",
        "precision = precision_score(y[test_idx], y_pred, average='weighted')\n",
        "recall = recall_score(y[test_idx], y_pred, average='weighted')\n",
        "f1 = f1_score(y[test_idx], y_pred, average='weighted')\n",
        "classification_result = {\n",
        "    'precision': precision, 'recall': recall, 'f1': f1\n",
        "}\n",
        "print(\"Classification (TF-IDF+LogReg):\", classification_result)\n"
      ],
      "metadata": {
        "id": "QROnaMpECuIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Content-based using Sentence-Transformer embeddings\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "user_profile_text = 'technology sports health entertainment business'\n",
        "user_embedding = embedder.encode(user_profile_text)\n",
        "article_embeddings = [embedder.encode(art['cleaned_text']) for cat in CATEGORIES for art in news_data[cat]]\n",
        "\n",
        "# Content-based scores: cosine similarity to user embedding\n",
        "content_scores = util.cos_sim(user_embedding, article_embeddings).flatten().numpy()\n",
        "\n",
        "# Collaborative filtering (SVD matrix factorization)\n",
        "# The 'ratings' matrix correctly represents (categories, articles)\n",
        "ratings = np.zeros((len(CATEGORIES), len(article_embeddings)))\n",
        "for i, cat in enumerate(CATEGORIES):\n",
        "    all_articles_flat_list = [article_obj for cat2 in CATEGORIES for article_obj in news_data[cat2]]\n",
        "    ratings[i, :] = [1.0 if article_obj in news_data[cat] else 0 for article_obj in all_articles_flat_list]\n",
        "\n",
        "svd = TruncatedSVD(n_components=5, random_state=42)\n",
        "svd.fit(ratings) # Fit SVD model to the ratings matrix\n",
        "\n",
        "# svd.components_ provides the latent features of articles, shape (n_components, num_articles)\n",
        "# Sum or average these components to get a single collaborative score per article\n",
        "collab_scores = svd.components_.mean(axis=0) # This will yield a (num_articles,) array\n",
        "\n",
        "# Hybrid recommendation: weighted sum\n",
        "hybrid_scores = 0.6 * content_scores + 0.4 * collab_scores\n",
        "rec_indices = np.argsort(hybrid_scores)[::-1][:5]\n",
        "top_recs = [all_texts[i] for i in rec_indices]"
      ],
      "metadata": {
        "id": "qzPyYy_XC1iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE evaluation between TextRank, BART, and Hybrid summaries\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_results = {}\n",
        "\n",
        "for cat in CATEGORIES:\n",
        "    art = news_data[cat][0]\n",
        "    ref = art['cleaned_text']  # Use preprocessed article as reference\n",
        "    smry = art['summaries']['hybrid']\n",
        "    scores = scorer.score(ref, smry)\n",
        "    rouge_results[cat] = {k: v.fmeasure for k, v in scores.items()}\n",
        "\n",
        "# Print classification metrics (already calculated)\n",
        "print(\"ROUGE results by category:\", rouge_results)\n",
        "print(\"Classification metrics:\", classification_result)\n"
      ],
      "metadata": {
        "id": "RSpbYKBWDXcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for all articles\n",
        "wc_text = ' '.join([art['cleaned_text'] for cat in CATEGORIES for art in news_data[cat]])\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(wc_text)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud of News Corpus\")\n",
        "plt.show()\n",
        "\n",
        "# Distribution plots: number of articles per category\n",
        "counts = [len(news_data[cat]) for cat in CATEGORIES]\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(CATEGORIES, counts, color='skyblue')\n",
        "plt.title('News Articles per Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZoqA6qMVDcAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n------ Academic-style Results Summary ------\")\n",
        "print(\"Number of articles fetched per category:\")\n",
        "for cat in CATEGORIES:\n",
        "    print(f\"- {cat}: {len(news_data[cat])}\")\n",
        "\n",
        "print(\"\\nExample Hybrid Summary (Technology):\")\n",
        "print(news_data['technology'][0]['summaries']['hybrid'])\n",
        "\n",
        "print(\"\\nExtracted Keywords (Technology, RAKE & TF-IDF):\")\n",
        "print(\"RAKE:\", news_data['technology'][0]['keywords']['rake'])\n",
        "print(\"TF-IDF:\", news_data['technology'][0]['keywords']['tfidf'])\n",
        "\n",
        "print(\"\\nNamed Entities (Technology):\")\n",
        "print(news_data['technology'][0]['ner'])\n",
        "\n",
        "print(\"\\nText Classification Result (Weighted):\")\n",
        "print(classification_result)\n",
        "\n",
        "print(\"\\nTop 5 Personalized Recommendations (hybrid):\")\n",
        "for idx, rec in enumerate(top_recs):\n",
        "    print(f\"{idx+1}. {rec[:250]}...\")  # Preview rec summaries\n",
        "\n",
        "print(\"\\nROUGE metrics (Hybrid summaries):\")\n",
        "for cat in CATEGORIES:\n",
        "    print(f\"{cat}: {rouge_results[cat]}\")\n",
        "\n",
        "print(\"\\nClassification metrics (Precision/Recall/F1):\")\n",
        "print(classification_result)\n",
        "\n",
        "print(\"\\n------ End of Output ------\")\n"
      ],
      "metadata": {
        "id": "WplnBGz4DghV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}